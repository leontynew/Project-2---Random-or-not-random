# -*- coding: utf-8 -*-
"""Project 2-Luyang Wang 65788904

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1AY_7YkzsGvnNXlc6y8hRLQJ1bLZqvjTa

# Random or not so random?

In the project, you will study the optimization algorithms related to the randomization. For a group of random algorithms, the randomness could be improved through certain strategy. The reference for this project comes from https://arxiv.org/pdf/1202.4184.pdf . 

The mathematical problem discussed in the above paper is quite simple, however it provides some insights for data science, especially for linear problems. The conjecture in the paper is still an open problem in general.

## First things first
We review the gradient descent method learned in last quarter, for minimization of $f(x)$, the Gradient Descent (GD) iteration is 
$$x_{k+1} = x_k - \alpha_k \nabla f(x_k)$$
with $\alpha_k$ the chosen step length, usually a backtracking type loop is associated for the choice of $\alpha_k$.

## Incremental GD
There are some variants of GD, like coordinate gradient descent or the stochastic gradient descent. Here we focus on the IGD: incremental GD, it is similar to the coordinate GD, which is dealing with something local and then deal with something else later. 

Suppose the problem is:
$$\min_x f(x) := \sum_{i=1}^m f_i(x)$$
the objective function is the summation of a few other objective functions. This is quite normal in real life applications. Especially the statistics and imaging science. 

The IGD is dealing with one of the $f_i$ with GD and then deal with others in similar manner. Suppose we have fixed an ordering $i_0, i_1, i_2, \dots, i_k,\dots$, this ordering is infinite or finite, the numbers are just sampled from $\{ 1,2,\dots, m\}$, basically telling you an order of $f_i$ to deal with. During each iteration, we update $x_k$ by

$$x_{k+1} = x_{k} - \gamma_k \nabla f_{i_k}(x_{k})$$
where $\gamma_k$ is the step length.  

When the ordering is sampled uniformly from $\{ 1,2,\dots, m\}$, then this method becomes **Stochastic GD**. But the ordering could be other kind or distribution, even deterministic, we will mainly focus on select a better strategy for the ordering.

## Implementation Task 1

This task comes from the Section 2.1 of the reference paper.

We implement the 1D example of least square problem for the IGD. Then problem is given by 

$$\min_x \frac{1}{2}\sum_{i=1}^n (x- y_i)^2$$
where $y_i$ is a sequence of scalars sampled with certain given mean value $\mu$ and variance $\sigma^2$. Following the above method, the iteration will be
$$x_{k+1} =x_k -\gamma_k (x_{k} - y_{i_k})$$
where $y_{i_k}$ comes from the selected ordering.  

In the first task, you need to implement the following functions.
"""

# generate a vector of random numbers which obeys the given distribution.
#
# n: length of the vector
# mu: mean value
# sigma: standard deviation.
# dist: choices for the distribution, you need to implement at least normal 
#       distribution and uniform distribution.
#
# For normal distribution, you can use ``numpy.random.normal`` to generate.
# For uniform distribution, the interval to sample will be [mu - sigma/sqrt(3), mu + sigma/sqrt(3)].

import numpy as np

def generate_random_numbers(n, mu, sigma, dist="normal"):
    # write your code here.
    if dist == "normal":
        return np.random.normal(mu, sigma, n)
    elif dist == "uniform":
        # write your code here.
        return np.random.uniform(mu-sigma/np.sqrt(3), mu+sigma/np.sqrt(3),n)
        
    else:
        raise Exception("The distribution {unknown_dist} is not implemented".format(unknown_dist=dist))
        
        
# test your code:
y_test = generate_random_numbers(5, 0, 0.1, "normal")

"""### Settings of the minimization problems
Now you will setup the problem with a sampling of $y$ vector with the following two different settings:
```
n, mu, sigma, dist = 105, 0.5, 1.0, "normal"
```
and 
```
n, mu, sigma, dist = 105, 0.5, 1.0, "uniform"
```
Your initial guess will be $x_0 = 0$ for both cases. The step length is taking the diminishing factor $\gamma_k = \frac{1}{k+1}$.
"""

y1 = generate_random_numbers(105, 0.5, 1.0, "normal")
y2 = generate_random_numbers(105, 0.5, 1.0, "uniform")

"""### Two orderings to experiment with

1. Random ordering with replacement
In this case, we do not choose the fixed ordering, we sample a number in $\{1,2,\dots, n \}$ each time and follow the iteration.

2. Random ordering without replacement
In this case, we do not choose the fixed ordering, we sample an ordering without replacement from $\{1,2,\dots, n\}$ and then follow the iteration.
"""

# IGD, the ordering is permitted to have replacement. 
#
#
def IGD_wr_task1(y):
    n = len(y)
    ordering = np.random.choice(n, n, replace=True)
    # implement the algorithm's iteration of IGD. Your result should return the the final xk
    # at the last iteration and also the history of objective function at each xk.
    x = np.zeros(n+1)
    for i in range(n):
      x[i+1] = x[i]-1/(i+1)*(x[i]-y[ordering[i]])
    return x


# IGD, the ordering is not permitted to have replacement.
#
#
def IGD_wo_task1(y):
    n = len(y)
    ordering = np.random.choice(n, n, replace=False)
    # implement the algorithm's iteration of IGD. Your result should return the the final xk
    # at the last iteration and also the history of objective function at each xk.
    x = np.zeros(n+1)
    for i in range(n):
      x[i+1] = x[i]-1/(i+1)*(x[i]-y[ordering[i]])
    return x

"""### Your results to present
Show your results by plotting two figures. For each cases of the above two settings, you need to plot the histories (``IGD_wr_task1`` and ``IGD_wo_task1``) of objective functions in the same figure. 

Conclude which strategy is better from the results. 

Briefly prove the ``IGD_wo_task1`` must converge to the true solution (mean value of ``y``). You can calculate first a few values of $x_k$ by hand to see why.
"""

# Commented out IPython magic to ensure Python compatibility.
import matplotlib.pyplot as plt
# %config InlineBackend.figure_format = 'svg'
# %matplotlib inline

x_wr1 = IGD_wr_task1(y1)
x_wo1 = IGD_wo_task1(y1)
n = len(x_wr1)
y_wr1 = np.zeros(n)
y_wo1 = np.zeros(n)
for i in range(n):
    y_wr1[i] = (np.linalg.norm(x_wr1[i]*np.ones(n-1)-y1))**2/2
    y_wo1[i] = (np.linalg.norm(x_wo1[i]*np.ones(n-1)-y1))**2/2
plt.figure
plt.plot(range(n),y_wr1,label='Random ordering with replacement')
plt.plot(range(n),y_wo1,label='Random ordering without replacement')
plt.xlabel('k')
plt.ylabel('objective function value with y1')
plt.legend()
plt.show()

x_wr2 = IGD_wr_task1(y2)
x_wo2 = IGD_wo_task1(y2)
n = len(x_wr2)
y_wr2 = np.zeros(n)
y_wo2 = np.zeros(n)
for i in range(n):
    y_wr2[i] = (np.linalg.norm(x_wr2[i]*np.ones(n-1)-y2))**2/2
    y_wo2[i] = (np.linalg.norm(x_wo2[i]*np.ones(n-1)-y2))**2/2
plt.figure
plt.plot(range(n),y_wr2,label='Random ordering with replacement')
plt.plot(range(n),y_wo2,label='Random ordering without replacement')
plt.xlabel('k')
plt.ylabel('objective function value with y2')
plt.legend()
plt.show()

"""From the above pictures, the strategy of Random ordering without replacement is better than the strategy of Random ordering with replacement; because it reaches the objective function value with less k values.

To show the IGD_wo_task1 must converge to the mean value of y, we first prove that $x_{k+1}$ is the mean of $y_{i_0}, \dots ,y_{i_k}$, that is to say $x_{k+1}=\frac{y_{i_0}+\dots+y_{i_k}}{k+1}$.

We prove it by induction. When k=0, $x_1=x_0-(x_0-y_{i_0})=y_{i_0}$, which is true. Assume it is true for k, then $x_{k+2}=x_{k+1}-\frac{x_{k+1}-y_{i_{k+1}}}{k+2}=\frac{y_{i_0}+\dots+y_{i_k}}{k+1}-\frac{\frac{y_{i_0}+\dots+y_{i_k}}{k+1}-y_{i_{k+1}}}{k+2}=\frac{y_{i_0}+\dots+y_{i_k+1}}{k+2}$, which it is true for k+1.

Thus it is true when k=n-1, i.e. $x_n=\frac{y_{i_0}+\dots+y_{i_{n-1}}}{n}$=mean of y. Therefore, the function converges to the true solution.

## Implementation Task 2

This task comes from the Section 2.1 of the reference paper as well. 

Instead of the problem in Task 1, we consider another problem, which is quite simple.

$$\min_x \frac{1}{2}\sum_{i=1}^n \beta_i (x - y)^2$$
here $\beta_i$ are positive weights, $y$ is a scalar. Clearly the minimum is at $x = y$. We try to use the IGD to solve the problem. 

$$x_{k+1} = x_k - \gamma_k \beta_{i_k} (x_k - y)$$

where $\gamma_k\equiv \gamma = 0.95 \min \beta_i^{-1}$. For simplicity, the $\beta_i$ are sampled uniformly between $[1,2]$. Your task2 is the same as the task1,  use two kinds of ordering to solve the problem and compare the results through figures.
"""

# IGD, the ordering is permitted to have replacement. 
#
#
def IGD_wr_task2(beta,y):
    n = len(beta)
    ordering = np.random.choice(n, n, replace=True)
    # implement the algorithm's iteration of IGD. Your result should return the the final xk
    # at the last iteration and also the history of objective function at each xk.
    x = np.zeros(n+1)
    gamma = 0.95*min(1./beta)
    for i in range(n):
        x[i+1] = x[i]-gamma*beta[ordering[i]]*(x[i]-y)
    return x


# IGD, the ordering is not permitted to have replacement.
#
#
def IGD_wo_task2(beta,y):
    n = len(beta)
    ordering = np.random.choice(n, n, replace=False)
    # implement the algorithm's iteration of IGD. Your result should return the the final xk
    # at the last iteration and also the history of objective function at each xk.
    x = np.zeros(n+1)
    gamma = 0.95*min(1./beta)
    for i in range(n):
        x[i+1] = x[i]-gamma*beta[ordering[i]]*(x[i]-y)
    return x

"""### Your results to present
Show your results by plotting two figures. For each cases of the above two settings, you need to plot the histories (``IGD_wr_task2`` and ``IGD_wo_task2``) of objective functions in the same figure. 

Conclude which strategy is better from the results. 
"""

#generate beta
beta = np.random.uniform(1,2,40)
y = 1.5 #choose y = 1.5
x_wr_task2 = IGD_wr_task2(beta,y)
x_wo_task2 = IGD_wo_task2(beta,y)
n = len(x_wr_task2)
y_wr_task2 = np.zeros(n)
y_wo_task2 = np.zeros(n)
for i in range(n):
    y_wr_task2[i] = (x_wr_task2[i]-y)**2/2*sum(beta)
    y_wo_task2[i] = (x_wo_task2[i]-y)**2/2*sum(beta)
plt.figure
plt.plot(range(n),y_wr_task2,label='Random ordering with replacement')
plt.plot(range(n),y_wo_task2,label='Random ordering without replacement')
plt.xlabel('k')
plt.ylabel('objective function value with y='+str(y))
plt.legend()
plt.show()

"""From the above picture, the strategy of Random ordering without replacement is better than the strategy of Random ordering with replacement; because this method reachs the minimum faster.

## Implementation Task 3

Now instead of 1D problem considered in above two tasks, we will consider the general case in higher dimensions, we study the least square problem to find solution of 
$$Ax = y$$
where $A$ is an $m\times n$ matrix and $x$ is a vector in $\mathbb{R}^n$. In most cases, $m \ge n$ which means the measurement is redundant for this linear model.  We denote the $i$th column $A^T$ as $a_i$ (namely the $i$th row of $A$ transposed), then we are minimizing

$$\min_x \sum_{i=1}^m (a_i^T x - y_i)^2$$
each $a_i\in\mathbb{R}^n$. 

And using IGD to minimize above problem with certain ordering. The step size $\gamma$ is  taken as a fixed constant, say $10^{-3}$, which is a common value for GD. 

Each iteration will be 

$$x_{k+1} = x_k - \gamma a_{i_k} (a_{i_k}^T x_{k} - y_{i_k})$$

Your initial guess is still all zeros vector.

### Generation of $y$
The $y$ vector is generated by taking the following form
$$y_i = a_i^T x^{\ast} + w_i$$
where $w_i$ obeys normal distribution of zero mean and variance of $\rho^2$, here $\rho = 10^{-2}$.
"""

# generation of exact solution and data y and matrix A.

def generate_problem_task3(m, n, rho):
    A = np.random.normal(0., 1.0, (m, n))
    x = np.random.random(n) # uniform in (0,1)
    w = np.random.normal(0., rho, m)
    y = A@x + w
    return A, x, y

# We generate the problem with 200x100 matrix. rho as 0.01.
#
A, xstar, y = generate_problem_task3(200, 100, 0.01)

# In these two functions, we could only focus on the first n steps and try to make comparisons on these data only.
# In practice, it requires more iterations to converge, due to the matrix might not be easy to deal with.
# You can put the ordering loop into a naive loop: namely, we simply perform the IGD code several rounds.
#
#
#
# IGD, the ordering is permitted to have replacement. 
#
#
def IGD_wr_task3(y, A, nloops):
    # implement the algorithm's iteration of IGD. Your result should return the the final xk
    # at the last iteration and also the history of objective function at each xk.
    ordering = np.array([],dtype=int)
    n = len(y)
    for i in range(nloops):
        ordering = np.concatenate((ordering,np.random.choice(n, n, replace=True)))
    nn = len(ordering)
    x = np.empty((nn+1,A.shape[1]))
    x[0] = np.zeros(A.shape[1])
    gamma = 0.001
    
    for i in range(nn):
        aik = A[ordering[i]]
        yik = y[ordering[i]]
        x[i+1] = x[i]-gamma*(aik@x[i]-yik)*aik
    return x


# IGD, the ordering is not permitted to have replacement.
#
#
def IGD_wo_task3(y, A, nloops):
    # implement the algorithm's iteration of IGD. Your result should return the the final xk
    # at the last iteration and also the history of objective function at each xk.
    ordering = np.array([],dtype=int)
    n = len(y)
    for i in range(nloops):
        ordering = np.concatenate((ordering,np.random.choice(n, n, replace=True)))
    nn = len(ordering)
    x = np.empty((nn+1,A.shape[1]))
    x[0] = np.zeros(A.shape[1])
    gamma = 0.001
    
    for i in range(nn):
        aik = A[ordering[i]]
        yik = y[ordering[i]]
        x[i+1] = x[i]-gamma*(aik@x[i]-yik)*aik
    return x

"""### Your results to present
Show your results by plotting two figures. For each cases of the above two settings, you need to plot the histories (``IGD_wr_task3`` and ``IGD_wo_task3``) of objective functions in the same figure.  

Then change your code to plot the histories of $\|x_k - x^{\ast }\|$ and compare the results about the convergences.

Conclude which strategy is better from the results. 
"""

nloops = 60
x_wr_task3 = IGD_wr_task3(y,A,nloops)
x_wo_task3 = IGD_wo_task3(y,A,nloops)
n = x_wr_task3.shape[0]
y_wr_task3 = np.zeros(n)
y_wo_task3 = np.zeros(n)
x_wr_xstar = np.zeros(n)
x_wo_xstar = np.zeros(n)
for i in range(n):
    y_wr_task3[i] = (np.linalg.norm(A@(x_wr_task3[i].T)-y))**2
    y_wo_task3[i] = (np.linalg.norm(A@(x_wo_task3[i].T)-y))**2
    x_wr_xstar[i] = np.linalg.norm(x_wr_task3[i]-xstar)
    x_wo_xstar[i] = np.linalg.norm(x_wo_task3[i]-xstar)
plt.figure
plt.plot(range(n),y_wr_task3,label='Random ordering with replacement')
plt.plot(range(n),y_wo_task3,label='Random ordering without replacement')
plt.xlabel('k')
plt.ylabel('objective function value')
plt.legend()
plt.show()

plt.figure
plt.plot(range(n),x_wr_xstar,label='Random ordering with replacement')
plt.plot(range(n),x_wo_xstar,label='Random ordering without replacement')
plt.xlabel('k')
plt.ylabel('norm of xk-xstar')
plt.legend()
plt.show()

"""From the above pictures, the strategy of Random ordering without replacement is better than the strategy of Random ordering with replacement; because this method reachs the minimum faster."""

